{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eca660e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6752d476",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9385901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\t  jupyter.o7280433    requirements.txt\tvenv\r\n",
      "checkpoints\t  jupyter.o7284871    runs\t\tvt-tr.o7146959\r\n",
      "data\t\t  logs\t\t      scripts\t\tvt-tr.o7209762\r\n",
      "jupyter.o7250076  notebooks\t      src\t\tvt-tr.o7242335\r\n",
      "jupyter.o7272725  out.txt\t      training\r\n",
      "jupyter.o7277176  prose_translations  translations\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "474d6d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9fcc833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_utils.batch import rebatch\n",
    "from src.data_utils.data import get_training_iterators\n",
    "from src.model.loss_optim import MultiGPULossCompute, SimpleLossCompute\n",
    "from src.model.model import make_model, NoamOpt, LabelSmoothing, translate_sentence\n",
    "from src.utils.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "853e827a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = get_tokenizer(\"tr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2094ab8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/auto/praha1/memduh/versetorch/venv/lib/python3.6/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/auto/praha1/memduh/versetorch/venv/lib/python3.6/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/auto/praha1/memduh/versetorch/venv/lib/python3.6/site-packages/torchtext/data/iterator.py:48: UserWarning: MyIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "train_iter, valid_iter, test_iter, train_idx, dev_idx, test_idx = get_training_iterators(\"tur\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a76160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fe2d480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini dev set\n",
    "with open(\"data/tr/tur.dev.tgt\", encoding=\"utf-8\") as infile:\n",
    "    toystrings = [x.strip() for x in infile.readlines()[:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0e9227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "toyset = [torch.LongTensor([1] + tok.Encode(x) + [2])  for x in toystrings]\n",
    "toyset = torch.nn.utils.rnn.pad_sequence(sequences=toyset, padding_value=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc0c8188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,     1,     1,  ...,     1,     1,     1],\n",
       "        [ 5605,     8,  1330,  ...,     8,   771,  2804],\n",
       "        [27861,  2475, 10284,  ...,  3987,  5057, 11694],\n",
       "        ...,\n",
       "        [    3,     3,     3,  ...,     3,     3,     3],\n",
       "        [    3,     3,     3,  ...,     3,     3,     3],\n",
       "        [    3,     3,     3,  ...,     3,     3,     3]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toyset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfa5a24",
   "metadata": {},
   "source": [
    "Two critics:\n",
    "- Input related to output or not\n",
    "- Classifier into poetry, prose, generated, scrambled poetry\n",
    "\n",
    "One word/token selector:\n",
    "- Choose tokens from input sequence to use for topic\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b709c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "import torchtext as tt\n",
    "from src.data_utils.batch import MyIterator\n",
    "from src.model.model import batch_size_val\n",
    "\n",
    "def each_line(fname):\n",
    "    c = 0\n",
    "    lines = []\n",
    "    with open(fname, \"r\", encoding=\"utf-8\") as infile:\n",
    "        for line in infile:\n",
    "            if line.count(\" \") > 200:\n",
    "                continue\n",
    "            lines.append(line.strip())\n",
    "            c += 1\n",
    "            if c >= 2000000: \n",
    "                break\n",
    "    return lines\n",
    "\n",
    "def make_iter(lines, tokenizer, batch_size=256):\n",
    "    \n",
    "    def tok(seq):\n",
    "        return tokenizer.EncodeAsIds(seq)\n",
    "\n",
    "    field = data.Field(tokenize=tok, init_token=1, eos_token=2, pad_token=3, use_vocab=False)\n",
    "    #ds = data.TabularDataset(fpath, \"tsv\", [(\"src\", field)], skip_header=True)\n",
    "\n",
    "    examples = [tt.data.Example.fromdict({\"src\": x}, {\"src\": (\"src\", field)}) for x in lines]\n",
    "    ds = tt.data.Dataset(examples, {\"src\": field})\n",
    "    iter = MyIterator(ds, batch_size=batch_size, device=\"cpu\",\n",
    "                             repeat=False, sort_key=lambda x: len(x.src),\n",
    "                             batch_size_fn=batch_size_val, train=False, sort=True)\n",
    "\n",
    "    return iter\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b8dcd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/auto/praha1/memduh/versetorch/venv/lib/python3.6/site-packages/torchtext/data/example.py:52: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "prose_iter = make_iter(each_line(\"data/tr/prose/prose_gan.txt\"), tok)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afb60edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "to_scramble = each_line(\"data/tr/tur.train.tgt\")\n",
    "scrambled = []\n",
    "for poem in to_scramble:\n",
    "    new_poem = poem.split(\"¬\")\n",
    "    random.shuffle(new_poem)\n",
    "    scrambled.append(\"¬\".join(new_poem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a86447bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrambled_iter = make_iter(scrambled, tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6a3757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbaa96c4",
   "metadata": {
    "scrolled": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d68c72a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bd4ad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from src.model.model import MultiHeadedAttention, PositionwiseFeedForward, \\\n",
    "                    PositionalEncoding, Encoder, EncoderLayer, Generator, Embeddings\n",
    "import torch.nn as nn\n",
    "\n",
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder, src_embed, generator):\n",
    "        super(Critic, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.src_embed = src_embed\n",
    "        self.generator = generator\n",
    "        self.steps = 0\n",
    "\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"Pass the input (and mask) through each layer in turn.\"\"\"\n",
    "        x = self.src_embed(x)\n",
    "        for layer in self.encoder.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.encoder.norm(x)    \n",
    "\n",
    "\n",
    "def make_critic(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"\"\"Helper: Construct a model from hyper-parameters.\"\"\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    generator = Generator(d_model, tgt_vocab)\n",
    "    embed = nn.Sequential(Embeddings(d_model, src_vocab), c(position))\n",
    "    encoder = Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N)\n",
    "    critic = Critic(encoder, embed, generator)\n",
    "    \n",
    "    # This was important from their code.\n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in critic.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform(p)\n",
    "\n",
    "    return critic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a01ac364",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/auto/plzen1/home/memduh/versetorch/src/model/model.py:264: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(p)\n",
      "/auto/praha1/memduh/versetorch/venv/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    }
   ],
   "source": [
    "ntokens = 32000\n",
    "enc_dec = make_model(ntokens, ntokens, N=6).to(device)\n",
    "token_selector = make_critic(ntokens, 2, N=2).to(device)\n",
    "style_critic = make_critic(ntokens, 4, N=2).to(device)\n",
    "relevance_critic = make_critic(ntokens + 1, 1, N=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d1f2655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0\n",
    "\n",
    "\n",
    "def prep_tensors( src, trg, pad=3):\n",
    "    src_mask = (src != pad).unsqueeze(-2)\n",
    "    trg_in = trg[:, :-1]\n",
    "    trg_y = trg[:, 1:]\n",
    "    trg_mask = make_std_mask(trg_in, pad)\n",
    "    return src, trg_y, src_mask, trg_mask\n",
    "\n",
    "def make_std_mask(tgt, pad):\n",
    "    \"\"\"Create a mask to hide padding and future words.\"\"\"\n",
    "    tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "    tgt_mask = tgt_mask & Variable(\n",
    "        subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "    return tgt_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e76a60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "feb11ab2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'it' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-5e5b47470deb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'it' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e1e475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f519b96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a93f0004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dae_input(tgt, token_selector):\n",
    "    select_prob_embeds = token_selector.forward(tgt.to(device), \n",
    "                                         (tgt != 3).unsqueeze(-2).to(device))\n",
    "    select_prob = token_selector.generator(select_prob_embeds)\n",
    "    select_indices = torch.max(select_prob, dim=2).indices.type(torch.ByteTensor)\n",
    "    dae_list = []\n",
    "    for ind, row in zip(select_indices, tgt):\n",
    "        dae_list.append(torch.masked_select(row, ind)[:15])\n",
    "    dae_input = torch.nn.utils.rnn.pad_sequence(dae_list, batch_first=False, padding_value=3)\n",
    "    return dae_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc47b15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bc8eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55acd3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "740266cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n",
      "Skipped overlong sample while batching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/auto/praha1/memduh/versetorch/venv/lib/python3.6/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<src.data_utils.batch.Batch at 0x14a78100fd68>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rebatched = (rebatch(3, b) for b in train_iter)\n",
    "next(rebatched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6aa72320",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(rebatched)\n",
    "p = next(iter(prose_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a323a19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/auto/praha1/memduh/versetorch/venv/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:733.)\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "b.trg\n",
    "dae_input = get_dae_input(b.trg, token_selector)\n",
    "dae_input_mask = dae_input != 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "548db73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.adafactor import Adafactor\n",
    "\n",
    "enc_dec_opt = NoamOpt(enc_dec.src_embed[0].d_model, 1, 2000,\n",
    "                        torch.optim.Adam(enc_dec.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "style_criterion = nn.BCELoss()\n",
    "relevance_criterion = nn.BCELoss()\n",
    "\n",
    "token_optim = Adafactor(token_selector.parameters())\n",
    "style_optim = Adafactor(style_critic.parameters())\n",
    "rel_optim = Adafactor(relevance_critic.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de7e3626",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4052e548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevance_input(dae_input, tgt):\n",
    "    mid_point = torch.ones((tgt.shape[0], 1), dtype=torch.long) * ntokens\n",
    "    return torch.cat((dae_input, mid_point.to(device), tgt), dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8c9115fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/auto/praha1/memduh/versetorch/venv/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:733.)\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "soft = torch.nn.Softmax(dim=1)\n",
    "for poetry_batch, prose_batch, scrambled_batch in zip(rebatched, prose_iter, scrambled_iter):\n",
    "    #zero_grad\n",
    "    enc_dec_opt.optimizer.zero_grad()\n",
    "    token_optim.zero_grad()\n",
    "    rel_optim.zero_grad()\n",
    "    style_optim.zero_grad()\n",
    "\n",
    "    \n",
    "    tgt, tgt_mask = poetry_batch.trg.to(device), poetry_batch.trg_mask.to(device)\n",
    "    # classify tokens, get the first 15 tokens selected.\n",
    "    dae_input = get_dae_input(poetry_batch.trg, token_selector).transpose(0,1).to(device)\n",
    "    # create src and src mask from selected tokens\n",
    "    dae_input_mask = (dae_input != 3).unsqueeze(-2)\n",
    "    # get output of poetry generator\n",
    "    output_embeds = enc_dec.forward(dae_input, tgt, dae_input_mask, tgt_mask)\n",
    "    # put through its generator, choose likeliest token\n",
    "    output = enc_dec.generator(output_embeds)\n",
    "    # torch.max that stuff\n",
    "    _, output_selected = torch.max(output, 2)\n",
    "    \n",
    "    #create rel critic input by concatenating dae input and tgt\n",
    "    rel_input = get_relevance_input(dae_input, tgt)\n",
    "    # get critic losses\n",
    "    style_scores = soft(style_critic.generator(style_critic.forward(output_selected.to(device), \n",
    "                                        (output_selected != 3).unsqueeze(-2).to(device)))[:,0,:])\n",
    "    relevance_scores = relevance_critic.generator(relevance_critic.forward(rel_input.to(device), \n",
    "                                        (rel_input != 3).unsqueeze(-2).to(device)))[:,0,:]\n",
    "    \n",
    "    style_loss = style_criterion(style_scores[:, 0], torch.ones((style_scores.shape[0])).to(device))\n",
    "    relevance_loss = relevance_criterion(relevance_scores.squeeze(), \n",
    "                                         torch.ones(relevance_scores.shape[0]).to(device))\n",
    "    enc_dec_loss = style_loss + relevance_loss\n",
    "    enc_dec_loss.backward()\n",
    "    enc_dec_opt.step()\n",
    "    token_optim.step()\n",
    "    \n",
    "    # samples and classes for style critic\n",
    "    # trg is poetry, prose batch is prose, model output is generated, scramble poetry lines for scrambled.\n",
    "    \n",
    "    # samples and classes for relevance critic\n",
    "    # rel_crit_loss = relevance_criterion(relevance_scores.squeeze(), \n",
    "    #                                     torch.ones(relevance_scores.shape[0]).to(device))\n",
    "    # scramb_rel_input = get_relevance_input(dae_input[torch.randperm(dae_input.shape[0]), :], tgt)\n",
    "    scrambled_relevance_scores = relevance_critic.generator(relevance_critic.forward(scramb_rel_input.to(device), \n",
    "                                        (scramb_rel_input != 3).unsqueeze(-2).to(device)))[:,0,:]\n",
    "    rel_crit_loss = relevance_criterion(scrambled_relevance_scores.squeeze(), \n",
    "                                         torch.zeros(scrambled_relevance_scores.shape[0]).to(device))\n",
    "    rel_crit_loss.backward()\n",
    "    rel_optim.step()\n",
    "    break \n",
    "    # dae_input - trg as input, 1 as output\n",
    "    # shuffled dae_input, trg as input, 0 as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8f6688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d392ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3d2307",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a786bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb4de57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6925e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "700729aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4, 18, 11,  7,  8, 16,  9,  3, 14,  5, 17,  1,  2,  6,  0, 19, 15, 13,\n",
       "        12, 10])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cc6ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones((8,19), dtype=torch.long) * 32000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "290d991b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  5605, 27861,  ...,     3,     3,     3],\n",
      "        [    1,  1330, 10284,  ...,     3,     3,     3],\n",
      "        [    1,   779,   850,  ...,     3,     3,     3],\n",
      "        ...,\n",
      "        [    1,   727,    36,  ...,     3,     3,     3],\n",
      "        [    1,     8, 29725,  ...,     3,     3,     3],\n",
      "        [    1,   771,  5057,  ...,     3,     3,     3]])\n"
     ]
    }
   ],
   "source": [
    "print(toyset.transpose(0, 1)[torch.randperm(toyset.shape[1]),:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d916387",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
