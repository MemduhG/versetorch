{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eca660e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6752d476",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9385901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\t\t\t       tur-baseline-211299-prose.sh.o7747867\r\n",
      "checkpoints\t\t\t       tur-baseline-211299.sh.o7747866\r\n",
      "data\t\t\t\t       tur-baseline-214186-prose.sh.o7747915\r\n",
      "logs\t\t\t\t       tur-baseline-214186.sh.o7747914\r\n",
      "news_translations\t\t       tur-baseline-217071-prose.sh.o7748183\r\n",
      "notebooks\t\t\t       tur-baseline-217071.sh.o7748182\r\n",
      "out.txt\t\t\t\t       tur-baseline-219958-prose.sh.o7748221\r\n",
      "prose_translations\t\t       tur-baseline-219958.sh.o7748220\r\n",
      "requirements.txt\t\t       tur-baseline-22210.sh.o7737186\r\n",
      "runs\t\t\t\t       tur-baseline-222814-prose.sh.o7748280\r\n",
      "scripts\t\t\t\t       tur-baseline-222814.sh.o7748279\r\n",
      "src\t\t\t\t       tur-baseline-225645-prose.sh.o7748341\r\n",
      "translations\t\t\t       tur-baseline-225645.sh.o7748340\r\n",
      "tur-baseline-1.sh.o7735992\t       tur-baseline-228485-prose.sh.o7748379\r\n",
      "tur-baseline-100647.sh.o7740203        tur-baseline-228485.sh.o7748378\r\n",
      "tur-baseline-103479.sh.o7740349        tur-baseline-231331-prose.sh.o7749136\r\n",
      "tur-baseline-106300.sh.o7740423        tur-baseline-231331.sh.o7749135\r\n",
      "tur-baseline-109138.sh.o7740453        tur-baseline-234194-prose.sh.o7749195\r\n",
      "tur-baseline-111953.sh.o7740495        tur-baseline-234194.sh.o7749194\r\n",
      "tur-baseline-11208.sh.o7736154\t       tur-baseline-237055-prose.sh.o7749230\r\n",
      "tur-baseline-114783.sh.o7740550        tur-baseline-237055.sh.o7749229\r\n",
      "tur-baseline-117617.sh.o7740571        tur-baseline-239928-prose.sh.o7749271\r\n",
      "tur-baseline-120449.sh.o7740611        tur-baseline-239928.sh.o7749270\r\n",
      "tur-baseline-123298.sh.o7740646        tur-baseline-242794-prose.sh.o7749312\r\n",
      "tur-baseline-126130.sh.o7740682        tur-baseline-242794.sh.o7749311\r\n",
      "tur-baseline-128964.sh.o7740694        tur-baseline-245651-prose.sh.o7749365\r\n",
      "tur-baseline-131784.sh.o7740766        tur-baseline-245651.sh.o7749364\r\n",
      "tur-baseline-134609.sh.o7740790        tur-baseline-248482-prose.sh.o7749467\r\n",
      "tur-baseline-137469.sh.o7740813        tur-baseline-248482.sh.o7749466\r\n",
      "tur-baseline-140293.sh.o7740852        tur-baseline-25044.sh.o7737270\r\n",
      "tur-baseline-14051.sh.o7736212\t       tur-baseline-251318-prose.sh.o7749500\r\n",
      "tur-baseline-143065.sh.o7740918        tur-baseline-251318.sh.o7749499\r\n",
      "tur-baseline-145693.sh.o7740956        tur-baseline-254160-prose.sh.o7749537\r\n",
      "tur-baseline-148493.sh.o7741021        tur-baseline-254160.sh.o7749536\r\n",
      "tur-baseline-151020-prose.sh.o7746451  tur-baseline-257023-prose.sh.o7749584\r\n",
      "tur-baseline-151020.sh.o7746450        tur-baseline-257023.sh.o7749583\r\n",
      "tur-baseline-153893-prose.sh.o7746508  tur-baseline-259890-prose.sh.o7749795\r\n",
      "tur-baseline-153893.sh.o7746507        tur-baseline-259890.sh.o7749794\r\n",
      "tur-baseline-156759-prose.sh.o7746897  tur-baseline-262743-prose.sh.o7749887\r\n",
      "tur-baseline-156759.sh.o7746896        tur-baseline-262743.sh.o7749886\r\n",
      "tur-baseline-159627-prose.sh.o7746927  tur-baseline-265572-prose.sh.o7749926\r\n",
      "tur-baseline-159627.sh.o7746926        tur-baseline-268404-prose.sh.o7749966\r\n",
      "tur-baseline-162484-prose.sh.o7746968  tur-baseline-271249-prose.sh.o7749996\r\n",
      "tur-baseline-162484.sh.o7746967        tur-baseline-27844.sh.o7737385\r\n",
      "tur-baseline-165345-prose.sh.o7746997  tur-baseline-2790.sh.o7736025\r\n",
      "tur-baseline-165345.sh.o7746996        tur-baseline-30625.sh.o7737515\r\n",
      "tur-baseline-168211-prose.sh.o7747068  tur-baseline-33419.sh.o7737635\r\n",
      "tur-baseline-168211.sh.o7747067        tur-baseline-36223.sh.o7737704\r\n",
      "tur-baseline-16863.sh.o7736248\t       tur-baseline-39035.sh.o7737866\r\n",
      "tur-baseline-171091-prose.sh.o7747109  tur-baseline-41838.sh.o7738327\r\n",
      "tur-baseline-171091.sh.o7747108        tur-baseline-44631.sh.o7738401\r\n",
      "tur-baseline-173967-prose.sh.o7747133  tur-baseline-47468.sh.o7738524\r\n",
      "tur-baseline-173967.sh.o7747132        tur-baseline-50253.sh.o7738594\r\n",
      "tur-baseline-176842-prose.sh.o7747206  tur-baseline-53050.sh.o7738677\r\n",
      "tur-baseline-176842.sh.o7747205        tur-baseline-5576.sh.o7736058\r\n",
      "tur-baseline-179703-prose.sh.o7747337  tur-baseline-55843.sh.o7738752\r\n",
      "tur-baseline-179703.sh.o7747336        tur-baseline-58646.sh.o7738803\r\n",
      "tur-baseline-182575-prose.sh.o7747439  tur-baseline-61461.sh.o7738899\r\n",
      "tur-baseline-182575.sh.o7747438        tur-baseline-64275.sh.o7738994\r\n",
      "tur-baseline-185426-prose.sh.o7747464  tur-baseline-67078.sh.o7739054\r\n",
      "tur-baseline-185426.sh.o7747463        tur-baseline-69825.sh.o7739080\r\n",
      "tur-baseline-188284-prose.sh.o7747578  tur-baseline-72622.sh.o7739130\r\n",
      "tur-baseline-188284.sh.o7747577        tur-baseline-75422.sh.o7739163\r\n",
      "tur-baseline-191147-prose.sh.o7747621  tur-baseline-78256.sh.o7739185\r\n",
      "tur-baseline-191147.sh.o7747620        tur-baseline-81075.sh.o7739242\r\n",
      "tur-baseline-19372.sh.o7737001\t       tur-baseline-83856.sh.o7739276\r\n",
      "tur-baseline-194014-prose.sh.o7747650  tur-baseline-8389.sh.o7736119\r\n",
      "tur-baseline-194014.sh.o7747649        tur-baseline-86637.sh.o7739355\r\n",
      "tur-baseline-196887-prose.sh.o7747689  tur-baseline-89422.sh.o7739501\r\n",
      "tur-baseline-196887.sh.o7747688        tur-baseline-92208.sh.o7739546\r\n",
      "tur-baseline-199749-prose.sh.o7747722  tur-baseline-95006.sh.o7739708\r\n",
      "tur-baseline-199749.sh.o7747721        tur-baseline-97830.sh.o7740126\r\n",
      "tur-baseline-202635-prose.sh.o7747766  venv\r\n",
      "tur-baseline-202635.sh.o7747765        vt-tr.o7735678\r\n",
      "tur-baseline-205522-prose.sh.o7747801  vt-tr.o7735818\r\n",
      "tur-baseline-205522.sh.o7747800        vt-tr.o7735865\r\n",
      "tur-baseline-208411-prose.sh.o7747832  vt-tr.o7735985\r\n",
      "tur-baseline-208411.sh.o7747831        vt-tr.o7736942\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "474d6d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9fcc833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_utils.batch import rebatch\n",
    "from src.data_utils.data import get_training_iterators\n",
    "from src.model.loss_optim import MultiGPULossCompute, SimpleLossCompute\n",
    "from src.model.model import make_model, NoamOpt, LabelSmoothing, translate_sentence\n",
    "from src.utils.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "853e827a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = get_tokenizer(\"tr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2094ab8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/auto/praha1/memduh/versetorch/venv/lib/python3.6/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/auto/praha1/memduh/versetorch/venv/lib/python3.6/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/auto/praha1/memduh/versetorch/venv/lib/python3.6/site-packages/torchtext/data/iterator.py:48: UserWarning: MyIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "train_iter, valid_iter, test_iter, train_idx, dev_idx, test_idx = get_training_iterators(\"tur\", batch_size=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a76160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fe2d480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini dev set\n",
    "with open(\"data/tr/tur.dev.tgt\", encoding=\"utf-8\") as infile:\n",
    "    toystrings = [x.strip() for x in infile.readlines()[:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0e9227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "toyset = [torch.LongTensor([1] + tok.Encode(x) + [2])  for x in toystrings]\n",
    "toyset = torch.nn.utils.rnn.pad_sequence(sequences=toyset, padding_value=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc0c8188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,     1,     1,  ...,     1,     1,     1],\n",
       "        [ 5605,     8,  1330,  ...,     8,   771,  2804],\n",
       "        [27861,  2475, 10284,  ...,  3987,  5057, 11694],\n",
       "        ...,\n",
       "        [    3,     3,     3,  ...,     3,     3,     3],\n",
       "        [    3,     3,     3,  ...,     3,     3,     3],\n",
       "        [    3,     3,     3,  ...,     3,     3,     3]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toyset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfa5a24",
   "metadata": {},
   "source": [
    "Two critics:\n",
    "- Input related to output or not\n",
    "- Classifier into poetry, prose, generated, scrambled poetry\n",
    "\n",
    "One word/token selector:\n",
    "- Choose tokens from input sequence to use for topic\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b709c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "import torchtext as tt\n",
    "from src.data_utils.batch import MyIterator\n",
    "from src.model.model import batch_size_val\n",
    "\n",
    "def each_line(fname):\n",
    "    c = 0\n",
    "    lines = []\n",
    "    with open(fname, \"r\", encoding=\"utf-8\") as infile:\n",
    "        for line in infile:\n",
    "            if line.count(\" \") > 200 or line.count(\" \") < 10:\n",
    "                continue\n",
    "            lines.append(line.strip())\n",
    "            c += 1\n",
    "            if c >= 2000000: \n",
    "                break\n",
    "    return lines\n",
    "\n",
    "def make_iter(lines, tokenizer, batch_size=256):\n",
    "    \n",
    "    def tok(seq):\n",
    "        return tokenizer.EncodeAsIds(seq)\n",
    "\n",
    "    field = data.Field(tokenize=tok, init_token=1, eos_token=2, pad_token=3, use_vocab=False)\n",
    "    #ds = data.TabularDataset(fpath, \"tsv\", [(\"src\", field)], skip_header=True)\n",
    "\n",
    "    examples = [tt.data.Example.fromdict({\"src\": x}, {\"src\": (\"src\", field)}) for x in lines]\n",
    "    ds = tt.data.Dataset(examples, {\"src\": field})\n",
    "    iter = MyIterator(ds, batch_size=batch_size, device=\"cpu\",\n",
    "                             repeat=False, sort_key=lambda x: len(x.src),\n",
    "                             batch_size_fn=batch_size_val, train=False, sort=True)\n",
    "\n",
    "    return iter\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8dcd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/auto/praha1/memduh/versetorch/venv/lib/python3.6/site-packages/torchtext/data/example.py:52: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "prose_iter = make_iter(each_line(\"data/tr/prose/prose_gan.txt\"), tok, batch_size=3000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb60edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "to_scramble = each_line(\"data/tr/tur.train.tgt\")\n",
    "scrambled = []\n",
    "for poem in to_scramble:\n",
    "    new_poem = poem.split(\"¬\")\n",
    "    random.shuffle(new_poem)\n",
    "    scrambled.append(\"¬\".join(new_poem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86447bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrambled_iter = make_iter(scrambled, tok, batch_size=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6a3757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbaa96c4",
   "metadata": {
    "scrolled": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d68c72a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd4ad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from src.model.model import MultiHeadedAttention, PositionwiseFeedForward, \\\n",
    "                    PositionalEncoding, Encoder, EncoderLayer, Generator, Embeddings\n",
    "import torch.nn as nn\n",
    "\n",
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder, src_embed, generator):\n",
    "        super(Critic, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.src_embed = src_embed\n",
    "        self.generator = generator\n",
    "        self.steps = 0\n",
    "\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"Pass the input (and mask) through each layer in turn.\"\"\"\n",
    "        x = self.src_embed(x)\n",
    "        for layer in self.encoder.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.encoder.norm(x)    \n",
    "\n",
    "\n",
    "def make_critic(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"\"\"Helper: Construct a model from hyper-parameters.\"\"\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    generator = Generator(d_model, tgt_vocab)\n",
    "    embed = nn.Sequential(Embeddings(d_model, src_vocab), c(position))\n",
    "    encoder = Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N)\n",
    "    critic = Critic(encoder, embed, generator)\n",
    "    \n",
    "    # This was important from their code.\n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in critic.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform(p)\n",
    "\n",
    "    return critic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01ac364",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = 32000\n",
    "enc_dec = make_model(ntokens, ntokens, N=6).to(device)\n",
    "token_selector = make_critic(ntokens, 2, N=2).to(device)\n",
    "style_critic = make_critic(ntokens, 4, N=2).to(device)\n",
    "relevance_critic = make_critic(ntokens + 1, 1, N=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1f2655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0\n",
    "\n",
    "\n",
    "def prep_tensors( src, trg, pad=3):\n",
    "    src_mask = (src != pad).unsqueeze(-2)\n",
    "    trg_in = trg[:, :-1]\n",
    "    trg_y = trg[:, 1:]\n",
    "    trg_mask = make_std_mask(trg_in, pad)\n",
    "    return src, trg_y, src_mask, trg_mask\n",
    "\n",
    "def make_std_mask(tgt, pad):\n",
    "    \"\"\"Create a mask to hide padding and future words.\"\"\"\n",
    "    tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "    tgt_mask = tgt_mask & Variable(\n",
    "        subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "    return tgt_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e76a60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb11ab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e1e475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f519b96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93f0004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dae_input(tgt, token_selector):\n",
    "    select_prob_embeds = token_selector.forward(tgt.to(device), \n",
    "                                         (tgt != 3).unsqueeze(-2).to(device))\n",
    "    select_prob = token_selector.generator(select_prob_embeds)\n",
    "    select_indices = torch.max(select_prob, dim=2).indices.type(torch.ByteTensor)\n",
    "    dae_list = []\n",
    "    for ind, row in zip(select_indices, tgt):\n",
    "        dae_list.append(torch.masked_select(row, ind)[:15])\n",
    "    dae_input = torch.nn.utils.rnn.pad_sequence(dae_list, batch_first=False, padding_value=3)\n",
    "    return dae_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc47b15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bc8eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55acd3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740266cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rebatched = (rebatch(3, b) for b in train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa72320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a323a19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548db73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.adafactor import Adafactor\n",
    "\n",
    "#enc_dec_opt = NoamOpt(enc_dec.src_embed[0].d_model, 1, 2000,\n",
    "#                        torch.optim.Adam(enc_dec.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "enc_dec_opt = Adafactor(enc_dec.parameters())\n",
    "\n",
    "style_criterion = nn.BCELoss()\n",
    "relevance_criterion = nn.BCELoss()\n",
    "\n",
    "token_optim = Adafactor(token_selector.parameters())\n",
    "style_optim = Adafactor(style_critic.parameters())\n",
    "rel_optim = Adafactor(relevance_critic.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563f360a",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9a064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevance_input(dae_input, tgt):\n",
    "    mid_point = torch.ones((tgt.shape[0], 1), dtype=torch.long) * ntokens\n",
    "    return torch.cat((dae_input, mid_point.to(device), tgt), dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8261d311",
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulation_steps = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f019d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get validation iterator\n",
    "\n",
    "\n",
    "def validate_batch(model, src, max_len=256, start_symbol=1, end_symbol=2):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    src_mask = (src != 3).unsqueeze(-2)\n",
    "    memory = model.encode(src.to(device), src_mask.to(device))\n",
    "    ys = torch.ones(src.shape[0], 1).fill_(start_symbol).type_as(src.data).to(device)\n",
    "    finished = torch.zeros((src.shape[0], 1))\n",
    "    for i in range(max_len-1):\n",
    "        out = model.decode(memory, src_mask,\n",
    "                           Variable(ys).to(device),\n",
    "                           Variable(subsequent_mask(ys.size(1)).type_as(src.data)).to(device))\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim = 1)\n",
    "        # next_word = next_word.data_utils[0]\n",
    "        unsqueezed = next_word.unsqueeze(1)\n",
    "        for c, token in enumerate(unsqueezed):\n",
    "            if token == end_symbol:\n",
    "                finished[c] = 1\n",
    "        if sum(finished) >= src.shape[0]:\n",
    "            break\n",
    "        ys = torch.cat([ys, unsqueezed], dim=1)\n",
    "                        # torch.ones(src.shape[0], 1).type_as(src.data_utils).fill_(next_word).to(device)], dim=1)\n",
    "    return ys\n",
    "\n",
    "\n",
    "def validate(model, selector, iterator):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0897944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_smoothing = LabelSmoothing(size=32000, padding_idx=3, smoothing=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673eecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_generate(model, src, max_len=256, start_symbol=1, end_symbol=2):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    src_mask = (src != 3).unsqueeze(-2)\n",
    "    memory = model.encode(src.to(device), src_mask.to(device))\n",
    "    ys = torch.ones(src.shape[0], 1).fill_(start_symbol).type_as(src.data).to(device)\n",
    "    finished = torch.zeros((src.shape[0], 1))\n",
    "    for i in range(max_len-1):\n",
    "        out = model.decode(memory, src_mask,\n",
    "                           Variable(ys).to(device),\n",
    "                           Variable(subsequent_mask(ys.size(1)).type_as(src.data)).to(device))\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim = 1)\n",
    "        # next_word = next_word.data_utils[0]\n",
    "        unsqueezed = next_word.unsqueeze(1)\n",
    "        for c, token in enumerate(unsqueezed):\n",
    "            if finished[c] == 1:\n",
    "                unsqueezed[c] = 3\n",
    "            if token == end_symbol:\n",
    "                finished[c] = 1\n",
    "        if sum(finished) >= src.shape[0]:\n",
    "            break\n",
    "        ys = torch.cat([ys, unsqueezed], dim=1)\n",
    "                        # torch.ones(src.shape[0], 1).type_as(src.data_utils).fill_(next_word).to(device)], dim=1)\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856c0faa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70729842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from src.utils.rhyme import critique_poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9115fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soft = torch.nn.Softmax(dim=1)\n",
    "start = time.time()\n",
    "all_tokens = 0\n",
    "for c, (poetry_batch, prose_batch, scrambled_batch) in enumerate(zip(rebatched, prose_iter, scrambled_iter)):\n",
    "    all_tokens += poetry_batch.ntokens\n",
    "    tgt, tgt_mask = poetry_batch.trg.to(device), poetry_batch.trg_mask.to(device)\n",
    "    # classify tokens, get the first 15 tokens selected.\n",
    "    src = poetry_batch.src.to(device)\n",
    "    # create src and src mask from selected tokens\n",
    "    src_mask = (src != 3).unsqueeze(-2)\n",
    "    \n",
    "    # get output of poetry generator\n",
    "    output_embeds = enc_dec.forward(src, tgt, src_mask, tgt_mask)\n",
    "    output = enc_dec.generator(output_embeds)\n",
    "    reconstruction_loss = label_smoothing(output.contiguous().view(-1, output.size(-1)),\n",
    "                             poetry_batch.trg_y.to(device).contiguous().view(-1)) / poetry_batch.ntokens\n",
    "    \n",
    "    \n",
    "    _, output_selected = torch.max(output, 2)\n",
    "    scores = [critique_poem(tok.Decode(x.tolist()), \"tr\", redif=True) \n",
    "              for x in output_selected]\n",
    "    rhyme_score = sum(x[0] for x in scores) / len(scores)\n",
    "    \n",
    "    reconstruction_loss += reconstruction_loss * (1 - rhyme_score)\n",
    "    \n",
    "    reconstruction_loss.backward()\n",
    "    token_optim.step() \n",
    "    enc_dec_opt.step()\n",
    "    enc_dec_opt.zero_grad()\n",
    "    \n",
    "    if c % 100  == 0:\n",
    "        print(\"Reconstruction loss:\", reconstruction_loss)\n",
    "        print(all_tokens / (time.time() - start), \"tokens processed per second.\")\n",
    "        if c% 500 == 0:\n",
    "            validated = greedy_generate(enc_dec, toyset.transpose(0, 1))\n",
    "            print([tok.Decode(x.tolist()) for x in validated])\n",
    "            print(scores)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8f6688",
   "metadata": {},
   "outputs": [],
   "source": [
    "toyset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d392ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3d2307",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a786bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb4de57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6925e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700729aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cc6ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290d991b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d916387",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
